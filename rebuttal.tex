%%
%% This is file `sample-authordraft.tex',
%% generated with the docstrip utility.
%%
%% The original source files were:
%%
%% samples.dtx  (with options: `authordraft')
%% 
%% IMPORTANT NOTICE:
%% 
%% For the copyright see the source file.
%% 
%% Any modified versions of this file must be renamed
%% with new filenames distinct from sample-authordraft.tex.
%% 
%% For distribution of the original source see the terms
%% for copying and modification in the file samples.dtx.
%% 
%% This generated file may be distributed as long as the
%% original source files, as listed above, are part of the
%% same distribution. (The sources need not necessarily be
%% in the same archive or directory.)
%%
%% Commands for TeXCount
%TC:macro \cite [option:text,text]
%TC:macro \citep [option:text,text]
%TC:macro \citet [option:text,text]
%TC:envir table 0 1
%TC:envir table* 0 1
%TC:envir tabular [ignore] word
%TC:envir displaymath 0 word
%TC:envir math 0 word
%TC:envir comment 0 0
%%
%%
%% The first command in your LaTeX source must be the \documentclass command.


\documentclass[sigconf,authordraft]{acmart}

\usepackage{multirow}
\usepackage{subfig}
\usepackage{float}
%% NOTE that a single column version may required for 
%% submission and peer review. This can be done by changing
%% the \doucmentclass[...]{acmart} in this template to 
%% \documentclass[manuscript,screen]{acmart}
%% 
%% To ensure 100% compatibility, please check the white list of
%% approved LaTeX packages to be used with the Master Article Template at
%% https://www.acm.org/publications/taps/whitelist-of-latex-packages 
%% before creating your document. The white list page provides 
%% information on how to submit additional LaTeX packages for 
%% review and adoption.
%% Fonts used in the template cannot be substituted; margin 
%% adjustments are not allowed.

%%
%% \BibTeX command to typeset BibTeX logo in the docs
\AtBeginDocument{%
  \providecommand\BibTeX{{%
    \normalfont B\kern-0.5em{\scshape i\kern-0.25em b}\kern-0.8em\TeX}}}



\settopmatter{printacmref=false} % Removes citation information below abstract
\renewcommand\footnotetextcopyrightpermission[1]{} % removes footnote with conference information in first column
\pagestyle{plain} % removes running headers 
\setcopyright{none}


%%
%% Submission ID.
%% Use this when submitting an article to a sponsored event. You'll
%% receive a unique submission ID from the organizers
%% of the event, and this ID should be used as the parameter to this command.
%%\acmSubmissionID{123-A56-BU3}

%%
%% For managing citations, it is recommended to use bibliography
%% files in BibTeX format.
%%
%% You can then either use BibTeX with the ACM-Reference-Format style,
%% or BibLaTeX with the acmnumeric or acmauthoryear sytles, that include
%% support for advanced citation of software artefact from the
%% biblatex-software package, also separately available on CTAN.
%%
%% Look at the sample-*-biblatex.tex files for templates showcasing
%% the biblatex styles.
%%

%%
%% For managing citations, it is recommended to use bibliography
%% files in BibTeX format.
%%
%% You can then either use BibTeX with the ACM-Reference-Format style,
%% or BibLaTeX with the acmnumeric or acmauthoryear sytles, that include
%% support for advanced citation of software artefact from the
%% biblatex-software package, also separately available on CTAN.
%%
%% Look at the sample-*-biblatex.tex files for templates showcasing
%% the biblatex styles.
%%

%%
%% The majority of ACM publications use numbered citations and
%% references.  The command \citestyle{authoryear} switches to the
%% "author year" style.
%%
%% If you are preparing content for an event
%% sponsored by ACM SIGGRAPH, you must use the "author year" style of
%% citations and references.
%% Uncommenting
%% the next command will enable that style.
%%\citestyle{acmauthoryear}

%%
%% end of the preamble, start of the body of the document source.
\begin{document}

%%
%% The "title" command has an optional parameter,
%% allowing the author to define a "short title" to be used in page headers.
\title{MVPbev: Multi-view Perspective Image Generation from BEV with Test-time Controllability and Generalizability}


%%
%% The "author" command and its associated commands are used to define
%% the authors and their affiliations.
%% Of note is the shared affiliation of the first two authors, and the
%% "authornote" and "authornotemark" commands
%% used to denote shared contribution to the research.
%%
%% By default, the full list of authors will be used in the page
%% headers. Often, this list is too long, and will overlap
%% other information printed in the page headers. This command allows
%% the author to define a more concise list
%% of authors' names for this purpose.


%%
%% This command processes the author and affiliation and title
%% information and builds the first part of the formatted document.
\newcommand{\issue}[1]{\vspace{0.1em}\noindent \textcolor{purple}{\textbf{#1 \hspace{0.2em}}}}
\newcommand{\idx}[1]{\hspace{0.2em}\textcolor{red}{\textbf{#1}}}

\newcommand{\rone}{\textcolor{orange}{\footnotesize R$\_$iFQi}}
\newcommand{\rtwo}{\textcolor{teal}{\footnotesize R$\_$JSaN}}
\newcommand{\rthree}{\textcolor{purple}{\footnotesize R$\_$etDR}}
\newcommand{\rfour}{\textcolor{pink}{\footnotesize R$\_$wUzG}}

\maketitle
We thank all four reviewers for their constructive comments and appreciation of the novelty (\rone, \rfour), flexibility, generalizability (\rtwo, \rthree, \rfour), and state-of-the-art performances (\rfour) of our MVPbev. %We will revise the grammar errors in our final version. 
Please see below for our feedback.
%Here, we will first address a commonly mentioned issue, and then we will respond to each reviewer's concerns about our work individually.


% \issue{Universal Issue: Ablation Study} As suggested by most reviewers (\textbf{iFQi}, \textbf{etDR} and \textbf{wUzG}), we conduct an additional ablation experiment to validate the effectiveness of the mechanisms/modules proposed in this work. We test our model with different setups on the validation set (using 1200 fixed samples), and results can be found in Table~\ref{tab_ablation_study}. We separately disable consistent initialization (marked as \textbf{Init} in Table~\ref{tab_ablation_study}), our special denoise mechanism that explicitly enforces cross-view consistency (\textbf{Denoise}) and multi-view attention module (\textbf{MVAttn}). The Model is evaluated using the metrics outlined in the experiment section of our main paper. We can find that our consistent initialization and special denoise mechanism mainly contribute to higher PSNR values, indicating better cross-view consistency. Additionally, the multi-view attention module enables our model to generate images with higher quality and semantic consistency since this parametric module increases the model’s capacity to better fit the dataset, and this module aggregates information for each view from their neighboring views, enhancing model's ability to understand scenes and yielding higher IoU scores.
% \vspace{0em}

\noindent \textbf{[\rone, \rthree, \rfour] Module-wise Ablation Study} We conduct new ablation studies on individual modules to validate the effectiveness and mutual informativeness of each component. Specifically, we explore the following three modules, namely consistent initialization (Init), multi-view attention module (MVAttn), and the denoise mechanism (Denoise), and report the results on validation set in Tab.~\ref{tab_ablation_study}. As expected, incorporating all modules tends to be the most reliable approach. Meanwhile, Init and Denoise prioritize cross-view consistency, resulting in improved PSNR in experiments. In contrast, MVAttn emphasizes semantic consistency, contributing to higher IoU and IS scores.

% \issue{Module-wise Ablation Study} As suggested by most reviewers (iFQi, etDR and wUzG), we conduct an ablation experiment to validate the effectiveness of the methods proposed in this work.

% \noindent\textbf{Experiment Setup}\hspace{0.5em} We test our model with different setups on the validation set (using 1200 fixed samples). Specifically, we separately disabled consistent initialization (marked as \textit{Init} in Table~\ref{tab_ablation_study}), our special denoise mechanism (\textit{Denoise}) and multi-view attention module (\textit{MVAttn}). Each setup is evaluated using the metrics outlined in the experiment section of our main paper and the quantitative results can be found in Table~\ref{tab_ablation_study}.

% \noindent\textbf{Analysis}\hspace{0.5em} We can find out that our consistent initialization and special denoise mechanism mainly contribute to higher PSNR values, indicating better cross-view consistency. Additionally, the multi-view attention module enables our model to generate images with higher quality and semantic consistency since this parametric module increases the model’s capacity to better fit the dataset, and this module aggregates information for each view from their neighboring views, enhancing model's ability to understand scenes and yielding higher IoU scores as a result.


\begin{table}[h]
\begin{tabular}{ccc|ccc|c|c}
\hline
\hspace{-0.35em}Init\hspace{-0.35em} & Denoise\hspace{-0.35em} & MVAttn & FID$\downarrow$ &  IS$\uparrow$ & CS$\uparrow$ & IoU$\uparrow$ & PSNR$\uparrow$\hspace{-0.35em} \\ \hline \hline
\checkmark & \checkmark & \checkmark & \textbf{17.20} & \underline{6.46} & \underline{28.79} & \textbf{0.508} & \underline{20.74} \\
\checkmark & \checkmark & & 18.65 & 5.84 & 28.59 & 0.478 & \textbf{20.84} \\
\checkmark &  & \checkmark & 19.72 & \textbf{6.78} & \textbf{28.85} & \underline{0.494} & 12.44 \\
 & \checkmark & \checkmark & \underline{18.42} & 6.27 & 28.78 & 0.490 & 20.70 \\ \hline
\end{tabular}
\caption{Module-wise ablation study. The best and second-best results are highlighted in bold and underline respectively.}
\label{tab_ablation_study}
\end{table}

%\issue{@Reviewer iFQi} Your concern about the ablation study is addressed above, demonstrating the effectiveness of our proposed methods. Thank you for your time!
\noindent \textbf{[\rone] Homography estimation} Our homography estimation follows the standard method described in~\cite{Hartley2004}.

% During our homography estimation process, the homography matirces are calculated based on intrinsic and extrinsic camera parameters, which is an indispensable step in our method. 

\noindent \textbf{[\rtwo] Controllability w.r.t. ControlNet and MVD} Neither ControlNet nor MVD is designed for cross-view scene generation, thereby we compared our approach only to limited SOTA methods, such as MagicDrive~\cite{gao2023magicdrive}, BEVGen~\cite{swerdlow2023street} and our revised ControlNet, to demonstrate our superiority. The test-time controllability and generalizability is achieved by our multi-instance control (l.498-512) and two-stage view-projection design (l.285-324), which exceeds any existing methods.

\noindent \textbf{[\rtwo] More details about test-time controllability} We will include details in Sec.2 of supplementary to the main paper in the final version. As noticed by \rtwo, our design extends beyond instance-level color control and can be applied to other signals that could be clearly represented by text, such as replacing "car" with "SUV". This approach is compatible with ControlNet and can also be integrated with other Diffusion-based methods. We will leave this to future directions/discussions.

\noindent \textbf{[\rtwo] Guarantee for test-time generalizability} Our test-time generalizability ensures the semantic consistency between BEV semantics and generated perspective image when view-point changes. This is achieved through our two-stage design, specifically the first view-projection stage (l.285-324) where geometry is exploited to enforce semantic consistency between BEV and perspective view. By decomposing the projection and generation process, MVPbev focuses on learning a view-invariant diffusion-model at the second stage, further allowing more generalizability during test-time.

% \issue{@Reviewer JSaN} We appreciate the time and effort you put into evaluating our work and we will address your concerns and questions one by one. \idx{i.} The instance controlability (mainly demonstrated through instance color controlability) relies on our proposed multi-objects control method, as introduced in supplementary material, and without that, existing works can hardly generate results with multiple finely controlled objects through text prompts. Therefore, we do not conduct comparative experiments regarding controlability. As for generalizability term, we do not compare Controlnet and MVD results because these two works do not originally focus on multi/street-view image generation. We re-implemented them for our task as a baseline, following a similar two-stage design. We belive it is the semantic consistent view projection in stage one guarantees generalizability to viewpoint variation. Consequently, we only compared our method with SOTA work, MagicDrive~\cite{gao2023magicdrive} which employs a different end-to-end design. 
% \idx{ii.} Due to page limitations, we included the introduction of instance control mechanism in the supplementary material. However, we will try to reorganize our text and move that section to the main paper in the revision. This mechanism can control elements beyond colors, because this training-free mechanism are actually utilizing the generalizability of pretrained diffusion model. Any feature that can be described by text can be controlled, as long as the diffusion model has the capability to generate the corresponding objects. Our object control mechanism operates entirely within the cross-attention layers (module-level), which are prevalent in both Diffusion model and Controlnet. Therefore, this mechanism can be and have been applied to Controlnet (please note that Controlnet requires text prompts as well). Additionally, our method, implemented based on the Diffuser library, can be easily transferred to any other network containing cross-attention layers to achieve multi-object control via text, as long as it accepts text as a condition.
% However, it might be inappropriate to apply this idea to Controlnet. Because, unlike semantic maps fed to Controlnet where control signals are naturally corresponded to their targets, the text prompts usually cannot control multiple objects accurately, that's why the motivation and key idea of this mechanism are to separately control multiple objects with text prompts. 
% \idx{iii.} Our test-time generalizability to view-point variation is guaranteed by our semantic-consistent view projection in stage one, which ensures generation process in following stage will always receive relatively accurate semantics when BEV semantics/camera setups vary. What's more, our model can quickly reach convergence because the BEV projection acts as a prior knowledge simplifying model learning, thanks to our two-stage design. This minimal tuning strategy (only finetuning the diffusion model while leaving Controlnet untouched) preserves the generalizability of both diffusion model and Controlnet. That's why our models are capable to generalize to unseen inputs.
% It's true that the generalizability of our model is not guaranteed by any specific mechanism. However, we believe it is our minimal tuning strategy (only finetuning the diffusion model while leaving Controlnet untouched) that preserves the generalizability of both diffusion model and Controlnet. Thanks to our two-stage design, our model can quickly reach convergence because the BEV projection acts as a prior knowledge simplifying model learning. Compared to the numerous training iterations required by the SOTA work~\cite{gao2023magicdrive}, our method can generate competitive results with much less training.

\noindent \textbf{[\rthree] Meaning of parameter $\mathbf{c}$} 
$\mathbf{c}$ consists of text-prompt and semantics in perspective view $S_m$ (l.456-458).
%Sorry for the confusing. $\mathbf{c}$ in Eq.\,2 represents the condition for condition encoder (l.). In our case, it consists of text-prompt and semantics in perspective view $S_m$ obtained from BEV semantics $B$.

\noindent \textbf{[\rthree] Results explanation of Tab.1 and Fig.10} Tab.1 consists of two sub-tables, aiming to showcase that MVPbev is training efficient and superior to SOTA methods (l.726-730). Moreover, it also gives far better performance under human analysis (l.908-917). Fig.10 demonstrates our instance-level color control and is briefly described in l.862-900 and our supplementary (Sec.3.3). We will provide more descriptions for them in the final version. 

\noindent \textbf{[\rthree] Comparison to MVD and ControlNet} Please check Tab.1 for our comparisons to all four baselines, including MagicDrive, BEVGen, MVD and ControlNet. And quantitative comparisons are provided in Fig.6. Details of baselines can be found in l.571-631. More implementation details are provided in Sec.4.2.

% \issue{@Reviewer etDR} \idx{i.} An ablation study is addressed above. \idx{ii.} The parameter $\mathbf{c}$ in Eq.\,2 is a condition for our conditional image generation which consists of text-prompt and semantics in perspective view $S_m$ obtained from BEV semantics $B$. 
% \idx{iii.} We start analyze our experimental results in Table.1 from Line 689. The advantages of our work are training efficient, training-free controlability on multiple instances and strong generalizability. We will summarize and highlight that in the revision. In Fig.\,10, the color difference (measured in Delta-E) between the color of generated objects and our reference color (as shown in box of the box plot) are visualized in a series of box plots. Notably, the samples located at the first quartile and the third quartile are highlighted. We will include these information in the final version. 

% \idx{iv.} The details of model's parameter can be found at Line 32 in supplementary materials and experimental environment are stated at Line 684 in our main paper. Our code will be made fully available. Thanks for your time!

\noindent \textbf{[\rfour] More analysis on limitations, especially on imperfect samples} Due to space limitation, we omit the discussions on the limitations of MVPbev. There are several areas that can be improved. First of all, the assumption of our view-projection can be invalid, such as with uphills or downhills. Meanwhile, better hyper-parameter learning method should be explored for better efficiency. We will include more imperfect samples and limitations in our final version.

\noindent \textbf{[\rfour] Typos} Thanks for pointing our typos out. We will fix them in our final version.

% \issue{@Reviewer wUzG} \idx{i.} Our method yields comparative/superior results w.r.t SOTA methods in terms of generation quality and semantic consistency with much less training required. Moreover, our method shows its strong controlability and generalizability which is not available in existing works. \idx{ii.} In our method, when the road surface is too uneven, the BEV projection results may become distorted. Additionally, the timestep of our special denoising mechanism is a manually set hyperparameter that cannot ensure perfect processing for all samples. We will include these analysis of our method's limitations in the final version of our paper. %\idx{iii.} An ablation study is presented above. \idx{iv.} We will fix typos and grammar errors in the revision. Thank you for your time and help in improving this work.

%%
%% The next two lines define the bibliography style to be used, and
%% the bibliography file.
\bibliographystyle{ACM-Reference-Format}
\bibliography{main}


\end{document}
\endinput
%%
%% End of file `sample-authordraft.tex'.
