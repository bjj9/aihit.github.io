\section{Introduction}
\label{sec:intro}
Multi-view perspective images are beneficial for autonomous driving tasks~\cite{caesar2020nuscenes}. Nowadays, multi-view cameras, including ones mounted in the front and on the side, have become basic requirements in large driving datasets, such as NuScenes~\cite{caesar2020nuscenes}, Argoverse~\cite{chang2019argoverse} and Waymo~\cite{sun2020scalability}. Typically, images from multiple camerasâ€™ views are perceived and further represented in Bird-Eye-View(BEV)~\cite{Wang_2019_CVPR}, where downstream tasks such as prediction and planning take place later on~\cite{Narayanan_2021_CVPR,Dauner2023CORL}. Intuitively, the BEV allows more interpretability as it provides a tangible interface to the real world, thus is beneficial and practical for higher-level modeling and decision making~\cite{Liu_2020_CVPR,Liu_2022_CVPR}. 

Though being of great importance in autonomous driving tasks, reliable BEV representation requires a large amount of data during the training stage, which can be time-consuming to obtain or annotate. One intuitive solution to this data issue is to obtain diverse perspective RGB images as well as their corresponding BEV semantics with generative models. 
% To this end, the simulation has been an effective method to address this data issue. 
Diverse yet plausible BEV semantics, compared to their corresponding perspective RGB or semantics, are much easier to simulate in a realistic manner with the help of parametric representations~\cite{Wang_2019_CVPR}. To this end, it is natural and practical to assume that BEV semantics, rather than perspective RGB images, are given.
%a natural to resolve this problem is to generate perspective RGB with given BEV semantics. Then 
Then the remaining question is to generate cross-view visually and semantically consistent photorealistic RGB images with known BEV semantics.

Despite the progress of generative models with constraints~\cite{zhang2023adding}, there are three main drawbacks in existing attempts to address this cross-view image generation problem~\cite{Tang2023mvdiffusion,gao2023magicdrive,swerdlow2023street}. Firstly, existing frameworks rely heavily on the training samples, leading to unsatisfactory test-time controllability. For instance, changing camera poses or providing extra control on object instance is beyond prior art. Moreover, cross-view consistency is not well enforced, resulting in inconsistent visual effects in overlapping FOVs. Finally, no thorough human analysis is performed on image generation tasks, resulting in un-interpretable comparison results.

% First of all, existing methods suffer from dramatic changes in viewpoints, making it impossible to generate perspective images with control signals in BEV. Secondly, consistency is not well enforced in overlapping FOVs, leading to unsatisfactory and inconsistent visual results. Finally, no thorough human analysis is performed on image generation tasks, resulting in un-interpretable comparison results.

% Firstly, though providing quite consistent background stuff with noticeable interactions, controllability is lacking in autoregressive-based methods~\cite{??}. In the meantime, foreground objects are also handled poorly, reflecting the fact that delicate interactions are missing in this line of work. More recent work~\cite{??} aims to provide more control with the help of diffusion models~\cite{??}. However, it suffers from image inconsistency, especially for background contents at overlapping field-of-view (FoV) of multiple cameras. 

To this end, we propose a novel two-stage method MVPbev that aims to generate controllable multi-view perspective RGB images with given BEV semantics and text prompts by explicitly enforcing cross-view consistency (See Fig.~\ref{fig:teaser}). Unlike existing work that lacks the test-time generalizability, MVPbev further allows both view-point and detailed text prompt changes at test-time, providing satisfactory performances under human analysis without requiring additional training data. To achieve that, MVPbev consists of two stages, or view projection and scene generation stages. The former stage transforms the given BEV semantics to multiple perspective view w.r.t. camera parameters. On the one hand, it enforces global consistency across views with explicit geometric transformation. On the other hand, such a design decouples the two stages, allowing the second stage to better capture view-point-invariant properties. The second stage of MVPbev starts from a pre-trained stable diffusion (SD) model. By explicitly incorporating a cross-view consistent module, together with our noise initialization and de-noising processes design, it can produce multi-view visually consistent and photo-realistic images, especially at overlapping FOVs. To further improve the test-time generalizability on objects, our MVPbev handles foreground instances and background layout individually, leading to better controllability during inference.
% The former focuses on not only the visual styling of RGB images from multiple cameras but also the overall semantic layout consistency w.r.t. both BEV semantics and text prompts. The latter, on the other hand, considers contents at overlapping FOVs of multiple cameras. Specifically, the global semantic layout consistency is firstly achieved with strict geometry by projecting the BEV semantics to multiple perspective views with given camera parameters. Meanwhile, the global and local visual consistencies are leveraged with the help of our multi-view attention module, as well as the noise initialization and de-noising processes. 
%our image stitching module, where dataset-wise camera homography is estimated in training RGB images and then used to stitch perspective semantics when overlapped FOV happens. 
% Compared to existing work, our multi-view attention module implicitly models the interactions between overlapping views, leading to more consistent prediction at overlapping areas. Finally, our MVPbev processes perspective semantics in parallel with a pre-trained text-to-image diffusion model, leveraging special designs of initialization and de-noising to enforce local visual consistency explicitly. 

We validate our ideas on NuScenes~\cite{caesar2020nuscenes} and follow the standard split. In contrast to methods that focus on improvements over downstream tasks or semantic consistency, we include additional extensive human analysis, especially on visual consistency over overlapping FOVs, across multiple views, and test-time view point and text prompt changes. We demonstrate that our proposed method not only provides better test-time controllability and generalizability, but also gives high-quality cross-view RGB images.
% more cross-view consistent RGB both visually and semantically.
In short, our contribution can be summarized as follows:
\begin{itemize}
    \item A novel multi-view image generation method that is capable of producing semantically and visually consistent perspective RGB images from BEV semantics, with only thousands of images as training data.
    \item A more controllable yet extendable algorithm that gives realistic perspective RGB images.
    \item State-of-the-art performances on large driving datasets, with comprehensive human analysis\footnote{Our code, data and model can be found in \url{https://github.com/kkaiwwana/MVPbev}.}.
\end{itemize}
% \begin{packed_lefty_item}
% \item A novel multi-view image generation method that is capable of producing semantically and visually consistent perspective RGB images from BEV semantics, with only thousands of images as training data.

% \item A more controllable yet extendable algorithm that gives realistic perspective RGB images.

% \item State-of-the-art performances on large driving datasets, with comprehensive human analysis.

% \item We extend the existing NuScenes dataset with language descriptions for background.
% \end{packed_lefty_item}
